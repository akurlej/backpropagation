{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4c1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import  load_iris\n",
    "from sklearn import preprocessing\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1cf62",
   "metadata": {},
   "source": [
    "Will use iris sepal length + sepal width (ie index 0 & index 1). Grab 5 samples to train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b095d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "features2use = [0,1]\n",
    "seed_from_id = 57\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "def grab_training_samples(samples_features,samples_target,samples2grab,seed):\n",
    "    random.seed(seed)\n",
    "    indices = random.sample(range(0,len(samples_features)),samples2grab)\n",
    "    return samples_features[indices],samples_target[indices]\n",
    "\n",
    "def cnv_target_to_prob(targets,types=3):\n",
    "    target_probs = []\n",
    "    for target in targets:\n",
    "        target_probs.append([1 if x==target else 0 for x in range(types)])\n",
    "    return target_probs\n",
    "    \n",
    "def cost(truth,prediction):\n",
    "    #implemented as cross-entropy cost\n",
    "    #check to see that truth is in the desired form...\n",
    "    def cost_elem(ti,pi):\n",
    "        return sum([-t*np.log2(p) for t,p in zip(ti,pi)])\n",
    "    return sum([cost_elem(ti,pi) for ti,pi in zip(truth,prediction)])/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f35d3",
   "metadata": {},
   "source": [
    "# Grab the features corresponding to index0 + index1 (sepal length+sepal width). Normalize them via the min/max scaler, and convert the target classes from 0,1,2 to [1,0,0],[0,1,0],[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e5a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features,training_target = grab_training_samples(iris.data,iris.target,5,seed_from_id)\n",
    "#grab only indices of interest from feature set.\n",
    "training_features = [ft[features2use] for ft in training_features]\n",
    "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "training_features = minmax_scaler.fit_transform(training_features)\n",
    "training_target = cnv_target_to_prob(training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4779c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(inputs,weights):\n",
    "    if len(inputs)==len(weights):\n",
    "        return np.dot(weights,inputs)\n",
    "    else:\n",
    "        return np.dot(weights[0:-1],inputs)+weights[-1]\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def sigmoid_deriv(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def softmax(z,normalize=1):\n",
    "    #REMEMBER TO DIVIDE BY sum(softmax)!\n",
    "    return np.exp(z)/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2afe31",
   "metadata": {},
   "source": [
    "***Our network will be the following three layers:***\n",
    "* Input (3 neurons), take x1,x2,1 and feed them through\n",
    "* Hidden (3 neurons), each neuron takes x1,x2,1, multiplies by weight, and then passes them through a sigmoid activation\n",
    "* Output (3 neurons), each neuron takes h1,h2,h3 (ie a hidden node) and 1, multiplies by weight and then passes them through a softmax activation\n",
    "\n",
    "***Configured with random weights***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a7d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(n_inputs,hidden_nodes,output_nodes,seed_from_id):\n",
    "    network=list()\n",
    "    random.seed(seed_from_id)\n",
    "    #generate hidden\n",
    "    hidden_layer = [{'weights':[random.random() for __ in range(n_inputs+1)]} for n in range(hidden_nodes)]\n",
    "    network.append(hidden_layer)\n",
    "    #generate output\n",
    "    output_layer = [{'weights':[random.random() for __ in range(hidden_nodes+1)]} for n in range(output_nodes)]\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    for idx,__ in enumerate(network[0]):\n",
    "        network[0][idx][\"activate\"]=activate\n",
    "        network[0][idx][\"transfer\"]=sigmoid\n",
    "        network[0][idx][\"name\"]=\"0,{}\".format(idx)\n",
    "    for idx,__ in enumerate(network[1]):\n",
    "        network[1][idx][\"activate\"]=activate\n",
    "        network[1][idx][\"transfer\"]=softmax\n",
    "        network[1][idx][\"name\"]=\"1,{}\".format(idx)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e7a44",
   "metadata": {},
   "source": [
    "***For the creation of this network, we have a list of:***\n",
    "<br>\n",
    "<br>\n",
    "[NetworkLayer0;NetworkLayer1]\n",
    "<br>\n",
    "<br>\n",
    "***Each \"NetworkLayer\" is composed of a set of neurons, and each neuron contains:***\n",
    "* Weights\n",
    "* Activate Function\n",
    "* Transfer Function\n",
    "* Name\n",
    "* Output (ie, yj)\n",
    "* dwij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f11ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weights': [0.04256571358257688, 0.5896864504016538, 0.019310811347186485],\n",
       "   'activate': <function __main__.activate(inputs, weights)>,\n",
       "   'transfer': <function __main__.sigmoid(z)>,\n",
       "   'name': '0,0'},\n",
       "  {'weights': [0.5141261392922695, 0.9766461363238077, 0.29024680692058136],\n",
       "   'activate': <function __main__.activate(inputs, weights)>,\n",
       "   'transfer': <function __main__.sigmoid(z)>,\n",
       "   'name': '0,1'},\n",
       "  {'weights': [0.9742198470014191, 0.6360311719284631, 0.8839870868472304],\n",
       "   'activate': <function __main__.activate(inputs, weights)>,\n",
       "   'transfer': <function __main__.sigmoid(z)>,\n",
       "   'name': '0,2'}],\n",
       " [{'weights': [0.019670627013046116,\n",
       "    0.7356439915697156,\n",
       "    0.5167042010454891,\n",
       "    0.604345615005703],\n",
       "   'activate': <function __main__.activate(inputs, weights)>,\n",
       "   'transfer': <function __main__.softmax(z, normalize=1)>,\n",
       "   'name': '1,0'},\n",
       "  {'weights': [0.3779795944555536,\n",
       "    0.976973385429604,\n",
       "    0.6552766031176774,\n",
       "    0.7255780591443041],\n",
       "   'activate': <function __main__.activate(inputs, weights)>,\n",
       "   'transfer': <function __main__.softmax(z, normalize=1)>,\n",
       "   'name': '1,1'},\n",
       "  {'weights': [0.6832371539307494,\n",
       "    0.546362446652179,\n",
       "    0.8134170520870861,\n",
       "    0.61626862838399],\n",
       "   'activate': <function __main__.activate(inputs, weights)>,\n",
       "   'transfer': <function __main__.softmax(z, normalize=1)>,\n",
       "   'name': '1,2'}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = create_network(2,3,3,seed_from_id)\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96add72c",
   "metadata": {},
   "source": [
    "***For every iteration during training, we restart the propagation (ie, set output and dwij sums to 0)\n",
    "Additionally, the predict function will be how we propagate forward***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0cc74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restart_propagation(network):\n",
    "    for layer in network:\n",
    "        for neuron in layer:\n",
    "            neuron[\"dwij\"]=np.zeros(len(neuron[\"weights\"]))\n",
    "            \n",
    "#generate our prediction given our input\n",
    "def predict(network,inputs,softmax_normalize=1):\n",
    "    for layer in network:\n",
    "        #weights at this layer.\n",
    "        new_inputs=[]\n",
    "        for neuron in layer:\n",
    "            #weights for this neuron\n",
    "            z=neuron[\"activate\"](inputs,neuron[\"weights\"])\n",
    "            y=neuron[\"transfer\"](z)\n",
    "            neuron[\"output\"] = y\n",
    "            new_inputs.append(y)\n",
    "        inputs=new_inputs\n",
    "    if softmax_normalize:\n",
    "        #print(inputs)\n",
    "        input_sum = sum(inputs)\n",
    "        inputs = [val/input_sum for val in inputs]\n",
    "        for idx,neuron in enumerate(network[-1]):\n",
    "            neuron[\"output\"] = neuron[\"output\"]/input_sum\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf28524",
   "metadata": {},
   "source": [
    "# For backpropagation derivation, we want:\n",
    "* w = weights\n",
    "* y = output from activation\n",
    "* z = output from A*x+B\n",
    "\n",
    "$$\\frac{d(E)}{d(w_{ij})} = \\frac{d(E)}{d(y_{j})} * \\frac{d(y_{j})}{d(w_{ij})} = \\frac{d(E)}{d(y_{j})} * \\frac{d(y_{j})}{d(z_{j})} * \\frac{d(z{j})}{d(w_{ij})}$$\n",
    "\n",
    "We know that $$\\frac{d(z_{j})}{d(w_{ij})} = y_{j}$$ and know that $$\\frac{d(y_{j})}{d(z_{j})} = \\frac{d(activation)}{d(z_{j})}$$\n",
    "\n",
    "Which reduces\n",
    "$$\\frac{d(E)}{d(w_{ij})} = \\frac{d(E)}{d(y_{j})} * \\frac{d(activation)}{d(z_{j})} * y_{i}$$\n",
    "\n",
    "We know that for cross-entropy loss, $$\\frac{d(E)}{d(z_{j})} = p_{j} - t_{j}$$\n",
    "\n",
    "Therefore, our backpropagation @ the output is:\n",
    "$$\\frac{d(E)}{d(w_{ij})} = (p_j-t_j)y_{i}$$\n",
    "And for the hidden layers, we have:\n",
    "$$\\frac{d(E)}{d(w_{ij})} = [\\sum_{k = output}((p_k-t_k)(y_k)w_{kj})]f(z_j)(1-f(z_j))y_i$$\n",
    "Where $f(z_{j})$ is the sigmoid activation in the hidden layer, and $y_i$ is the input into the hidden node.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f0e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate_hidden_output(network,y):\n",
    "    #for backwards training, j = the output, i = the previous node\n",
    "    #p_j is softmax prediction, t_j is the corresponding index\n",
    "    for j,nj in enumerate(network[-1]):\n",
    "        de_dz=(nj[\"output\"]-y[j])\n",
    "        for i,ni in enumerate(network[-2]):\n",
    "            nj[\"dwij\"][i] += de_dz*ni[\"output\"]\n",
    "        nj[\"dwij\"][-1] += de_dz\n",
    "    \n",
    "    \n",
    "def backward_propagate_input_hidden(network,x):\n",
    "    #for backwards training, j=the output, i=the previous node\n",
    "    for j,nj in enumerate(network[-2]):\n",
    "        #need to grab the dwij from the previous node (call it k)\n",
    "        de_dz = 0\n",
    "        for k,nk in enumerate(network[-1]):\n",
    "            de_dz += nk[\"dwij\"][k]*nk[\"weights\"][j]\n",
    "        #now can set our dj\n",
    "        f_zj = nj[\"output\"]\n",
    "        for i,val in enumerate(x):\n",
    "            nj[\"dwij\"][i] += de_dz*(f_zj*(1-f_zj))*val\n",
    "        nj[\"dwij\"][-1] += de_dz*(f_zj*(1-f_zj))\n",
    "\n",
    "def update_weights(network, learningRate, N):\n",
    "    for layer in network:\n",
    "        for neuron in layer:\n",
    "            weights = neuron[\"weights\"]\n",
    "            dwij = neuron[\"dwij\"]\n",
    "            weights = weights - learningRate*(dwij/N)\n",
    "            neuron[\"weights\"] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596327ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD(network,epochs,learning_rate,X,Y):\n",
    "    for itr in range(epochs):\n",
    "        restart_propagation(network)\n",
    "        prediction = []\n",
    "        for x,y in zip(X,Y):\n",
    "            prediction.append(predict(network,x)) #/forward propagate\n",
    "            backward_propagate_hidden_output(network,y)\n",
    "            backward_propagate_input_hidden(network,x)\n",
    "        update_weights(network,learning_rate,len(Y))\n",
    "        print(\"Itr, XEB Cost = {}, {}\".format(itr,cost(Y,prediction)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd4b37",
   "metadata": {},
   "source": [
    "# (i) For calculation of XEB after two rounds of training, with the initial configuration of seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f196808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr, XEB Cost = 0, 1.6945662449154297\n",
      "Itr, XEB Cost = 1, 1.5999797078522264\n"
     ]
    }
   ],
   "source": [
    "BGD(network,2,0.5,training_features,training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a4918",
   "metadata": {},
   "source": [
    "# (ii) For how back propagation for hidden->output was calculated see below (and see above for the associated derivation)\n",
    "```\n",
    "def backward_propagate_hidden_output(network,y):\n",
    "    #for backwards training, j = the output, i = the previous node\n",
    "    #p_j is softmax prediction, t_j is the corresponding index\n",
    "    for j,nj in enumerate(network[-1]):\n",
    "        de_dz=(nj[\"output\"]-y[j])\n",
    "        for i,ni in enumerate(network[-2]):\n",
    "            nj[\"dwij\"][i] += de_dz*ni[\"output\"]\n",
    "        nj[\"dwij\"][-1] += de_dz\n",
    "    \n",
    "```\n",
    "# (iii) For how back propagation for input->hidden was calculated, see below (and see above for the associated derivation)\n",
    "```\n",
    "def backward_propagate_input_hidden(network,x):\n",
    "    #for backwards training, j=the output, i=the previous node\n",
    "    for j,nj in enumerate(network[-2]):\n",
    "        #need to grab the dwij from the previous node (call it k)\n",
    "        de_dz = 0\n",
    "        for k,nk in enumerate(network[-1]):\n",
    "            de_dz += nk[\"dwij\"][k]*nk[\"weights\"][j]\n",
    "        #now can set our dj\n",
    "        f_zj = nj[\"output\"]\n",
    "        for i,val in enumerate(x):\n",
    "            nj[\"dwij\"][i] += de_dz*(f_zj*(1-f_zj))*val\n",
    "        nj[\"dwij\"][-1] += de_dz*(f_zj*(1-f_zj))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428cd316",
   "metadata": {},
   "source": [
    "# (iv) For the bonus question, lets pick all four parameters. In this case, we simply run the following set of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1316c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features,training_target = grab_training_samples(iris.data,iris.target,5,seed_from_id)\n",
    "#grab only indices of interest from feature set.\n",
    "training_features = [ft for ft in training_features]\n",
    "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "training_features = minmax_scaler.fit_transform(training_features)\n",
    "training_target = cnv_target_to_prob(training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8975d0",
   "metadata": {},
   "source": [
    "***Need only to set #inputs = 4 and # of hidden neurons = 5***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5693cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr, XEB Cost = 0, 1.684429021348008\n",
      "Itr, XEB Cost = 1, 1.5975206772321364\n"
     ]
    }
   ],
   "source": [
    "network = create_network(4,5,3,3)\n",
    "BGD(network,2,0.5,training_features,training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157fc49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
